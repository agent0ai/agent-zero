#!/usr/bin/env python3

import argparse
import os
import sys
import subprocess
import time
from datetime import datetime

# Define constants
DEFAULT_OUTPUT_DIR = "/root/generated_images"
MODEL_CACHE_DIR = os.path.expanduser("~/.cache/stable-diffusion")

def install_requirements():
    """Install all required packages with compatible versions"""
    print("üîÑ Checking dependencies...")
    
    # First check what's already installed
    try:
        import torch
        print(f"‚úÖ PyTorch already installed: {torch.__version__}")
        pytorch_installed = True
        
        # Check if CUDA is available in the installed version
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            print(f"‚úÖ CUDA already available: {torch.cuda.get_device_name(0)}")
        else:
            print("‚ö†Ô∏è PyTorch is installed but CUDA is not available")
            # If PyTorch is installed without CUDA, uninstall and reinstall with CUDA
            subprocess.run([sys.executable, "-m", "pip", "uninstall", "-y", "torch", "torchvision", "torchaudio"])
            pytorch_installed = False
            
    except ImportError:
        print("‚ùå PyTorch not installed")
        pytorch_installed = False
        
    # Only install PyTorch if it's not already installed or was uninstalled due to missing CUDA
    if not pytorch_installed:
        print("üîÑ Installing PyTorch with CUDA support...")
        try:
            process = subprocess.run(
                [sys.executable, "-m", "pip", "install", "torch==2.6.0+cu124", "torchvision", "torchaudio", "--index-url", "https://download.pytorch.org/whl/cu124"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            if process.returncode == 0:
                print("‚úÖ Successfully installed PyTorch with CUDA support")
            else:
                print("‚ö†Ô∏è Failed to install PyTorch with CUDA, falling back to CPU version")
                subprocess.run(
                    [sys.executable, "-m", "pip", "install", "torch", "torchvision", "torchaudio"],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE
                )
        except Exception as e:
            print(f"‚ùå Error installing PyTorch: {e}")
    
    # First install core dependencies that others depend on
    core_dependencies = {
        "huggingface_hub": "0.20.3",
        "safetensors": "0.4.1",
        "accelerate": "0.21.0"
    }
    
    for package, version in core_dependencies.items():
        try:
            subprocess.run(
                [sys.executable, "-m", "pip", "uninstall", "-y", package],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            print(f"üîÑ Installing {package} version {version}")
            subprocess.run(
                [sys.executable, "-m", "pip", "install", f"{package}=={version}", "--quiet"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            print(f"‚úÖ {package} installed: {version}")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Issue with {package}: {e}")
            
    # Now install the main dependencies
    dependencies = {
        "diffusers": "0.25.0",  # Updated to a version compatible with Python 3.12
        "transformers": "4.38.2",
        "scipy": "1.15.2"
    }
    
    for package, version in dependencies.items():
        try:
            subprocess.run(
                [sys.executable, "-m", "pip", "uninstall", "-y", package],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            print(f"üîÑ Installing {package} version {version}")
            subprocess.run(
                [sys.executable, "-m", "pip", "install", f"{package}=={version}", "--quiet"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            print(f"‚úÖ {package} installed: {version}")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Issue with {package}: {e}")
    
    # Try to install xformers if CUDA is available
    try:
        import torch
        if torch.cuda.is_available():
            try:
                import xformers
                print(f"‚úÖ xformers already installed: {xformers.__version__}")
            except ImportError:
                print("üîÑ Installing xformers for better GPU performance...")
                try:
                    subprocess.run(
                        [sys.executable, "-m", "pip", "install", "xformers==0.0.29.post3", "--quiet"],
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE
                    )
                    print("‚úÖ Successfully installed xformers")
                except Exception as e:
                    print(f"‚ö†Ô∏è Warning: Failed to install xformers: {e}")
                    print("‚ö†Ô∏è This is not critical, generation will work without it.")
    except Exception:
        pass
        
    print("‚úÖ Dependency check complete")
    return True

def check_cuda():
    """Check if CUDA is available"""
    try:
        # Check if nvidia-smi command works
        nvidia_smi = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=5)
        if nvidia_smi.returncode == 0:
            print("‚úÖ NVIDIA GPU detected via nvidia-smi")
            return True
            
        return False
    except:
        return False

def print_versions():
    """Print installed versions of all relevant packages"""
    print("\nüì¶ Installed Package Versions:")
    print("-" * 40)
    
    try:
        import torch
        print(f"PyTorch: {torch.__version__}")
        print(f"CUDA Available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"CUDA Version: {torch.version.cuda}")
            print(f"cuDNN Version: {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else 'Not available'}")
            print(f"GPU: {torch.cuda.get_device_name(0)}")
    except ImportError:
        print("PyTorch: Not installed")
    
    packages = [
        "diffusers",
        "transformers",
        "safetensors",
        "accelerate",
        "scipy",
        "xformers",
        "huggingface_hub"
    ]
    
    for package in packages:
        try:
            module = __import__(package)
            version = getattr(module, "__version__", "Unknown version")
            print(f"{package}: {version}")
        except ImportError:
            print(f"{package}: Not installed")
    
    print("-" * 40)

def generate_image(prompt, output_dir=DEFAULT_OUTPUT_DIR, seed=None, size=(512, 512)):
    """Generate an image using Stable Diffusion
    
    Args:
        prompt: Text prompt describing the image to generate
        output_dir: Directory to save the generated image
        seed: Random seed for reproducibility (optional)
        size: Output image size as (width, height) tuple (default: 512x512)
    """
    print(f"üñºÔ∏è Generating image with prompt: \"{prompt}\"")
    
    # Install dependencies
    install_requirements()
    
    # Import required libraries
    try:
        import torch
        from diffusers import StableDiffusionPipeline
        from PIL import Image
        
        # Force a CUDA availability check to ensure we have the latest status
        # After installing CUDA-enabled PyTorch, this should reflect the current state
        torch.cuda.is_available()  # This refreshes the CUDA status
        
        print(f"‚úÖ Using PyTorch {torch.__version__}")
        print(f"‚úÖ CUDA available: {torch.cuda.is_available()}")
        
        # First try to use CUDA if available
        if torch.cuda.is_available():
            device = "cuda"
            print(f"‚úÖ Using CUDA device: {torch.cuda.get_device_name(0)}")
            print(f"‚ÑπÔ∏è GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            # Check if NVIDIA GPU is physically available even if CUDA is not detected by PyTorch
            has_nvidia_gpu = check_cuda()
            if has_nvidia_gpu:
                print("üîÑ NVIDIA GPU detected but CUDA not available in PyTorch. Trying to use GPU anyway...")
                device = "cuda"
            else:
                device = "cpu"
                print("‚ö†Ô∏è Using CPU (CUDA not available)")
        
        # Set seed if provided
        if seed is not None:
            torch.manual_seed(seed)
            print(f"üé≤ Using seed: {seed}")
        
        # Load the model
        print("üîÑ Loading Stable Diffusion model...")
        start_time = time.time()
        
        try:
            # Load the model with appropriate settings
            pipe = StableDiffusionPipeline.from_pretrained(
                "stabilityai/stable-diffusion-2-1-base",
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                safety_checker=None,
                cache_dir=MODEL_CACHE_DIR,
                resume_download=True,
                use_safetensors=True
)
            
            # Try to move model to GPU if that's the intended device
            try:
                if device == "cuda":
                    pipe = pipe.to(device)
                    
                    # Enable memory optimizations
                    pipe.enable_attention_slicing()
                    
                    # Enable xformers if available and on CUDA
                    try:
                        import xformers
                        pipe.enable_xformers_memory_efficient_attention()
                        print("‚úÖ Using xformers for memory efficient attention")
                    except ImportError:
                        print("‚ö†Ô∏è xformers not available, using standard attention")
                else:
                    pipe = pipe.to(device)
                    pipe.enable_attention_slicing()
            except RuntimeError as e:
                if "CUDA" in str(e):
                    print(f"‚ö†Ô∏è Error moving model to CUDA: {e}")
                    print("‚ö†Ô∏è Falling back to CPU")
                    device = "cpu"
                    pipe = pipe.to(device)
                    pipe.enable_attention_slicing()
                    
                    # Check if we have an NVIDIA GPU but still had to fall back to CPU
                    if check_cuda():
                        print("üîÑ NOTE: NVIDIA GPU was detected but PyTorch couldn't use CUDA.")
                        print("üîÑ Try running the script again now that dependencies are installed.")
                        print("üîÑ A restart of your Python environment might be needed for CUDA to be properly recognized.")
                else:
                    raise
            
            print(f"‚úÖ Model loaded in {time.time() - start_time:.2f} seconds")
            
            # Create output directory if it doesn't exist
            os.makedirs(output_dir, exist_ok=True)
            
            # Generate the image
            print(f"üîÑ Generating image on {device}...")
            gen_start_time = time.time()
            
            # CUDA out of memory handling
            try:
                with torch.inference_mode():
                    output = pipe(
                        prompt=prompt,
                        num_inference_steps=50 if device == "cuda" else 20,
                        guidance_scale=9,
                        height=size[1],
                        width=size[0]
                    )
                    image = output.images[0]
                
                print(f"‚úÖ Image generated in {time.time() - gen_start_time:.2f} seconds")
            except RuntimeError as e:
                if "out of memory" in str(e).lower() and device == "cuda":
                    print("‚ö†Ô∏è CUDA out of memory. Moving to CPU...")
                    # Move to CPU and try again with smaller settings
                    pipe = pipe.to("cpu")
                    with torch.inference_mode():
                        output = pipe(
                            prompt=prompt,
                            num_inference_steps=20,  # Fewer steps
                            guidance_scale=7.0,     # Lower guidance scale
                            height=min(512, size[1]),  # Limit size
                            width=min(512, size[0])
                        )
                        image = output.images[0]
                    print(f"‚úÖ Image generated on CPU in {time.time() - gen_start_time:.2f} seconds")
                else:
                    # Re-raise other errors
                    raise
            
            # Save the image
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"image_{timestamp}.png"
            filepath = os.path.join(output_dir, filename)
            
            image.save(filepath)
            print(f"üíæ Image saved to: {filepath}")
            
            return filepath
            
        except Exception as e:
            print(f"‚ùå Error generating image: {e}")
            return None
        
    except ImportError as e:
        print(f"‚ùå Error importing required modules: {e}")
        return None

def main():
    parser = argparse.ArgumentParser(description="Generate images using Stable Diffusion")
    parser.add_argument("prompt", nargs="?", type=str, help="The prompt for image generation")
    parser.add_argument("--seed", type=int, default=None, help="Random seed for reproducibility")
    parser.add_argument("--output-dir", type=str, default=DEFAULT_OUTPUT_DIR, help="Directory to save generated images")
    parser.add_argument("--width", type=int, default=512, help="Width of the generated image (default: 512)")
    parser.add_argument("--height", type=int, default=512, help="Height of the generated image (default: 512)")
    args = parser.parse_args()
    
    # Check if a prompt was provided
    if not args.prompt:
        print("‚ùå No prompt provided. Please specify a prompt.")
        print("Example: python image_generation.py 'A majestic dragon'")
        return 1
    
    # Generate the image
    filepath = generate_image(
        args.prompt,
        args.output_dir,
        seed=args.seed,
        size=(args.width, args.height)
    )
    
    if filepath:
        print(f"‚ú® Image generation completed successfully!")
        print_versions()  # Print versions after successful generation
        
        # Add a message if we used CPU but have GPU hardware
        import torch
        if not torch.cuda.is_available() and check_cuda():
            print("\nüîÑ NOTE: This image was generated on CPU but an NVIDIA GPU was detected.")
            print("üîÑ CUDA support was installed successfully but requires a restart of the Python environment.")
            print("üîÑ Please run the script again to utilize GPU acceleration for faster image generation.")
        
        return 0
    else:
        print("‚ùå Image generation failed")
        # Check if we installed CUDA support and a GPU is available but CUDA wasn't recognized
        try:
            import torch
            if not torch.cuda.is_available() and check_cuda():
                print("\n‚úÖ CUDA support was installed successfully but requires a restart of the Python environment.")
                print("‚úÖ Please run the script again to utilize GPU acceleration for faster image generation.")
        except ImportError:
            pass
        return 1

if __name__ == "__main__":
    sys.exit(main())
