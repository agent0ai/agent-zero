# 零号行动增强模型系统配置示例
# 复制此文件为 .env 并根据您的环境进行配置

# ============================================================================
# OpenAI兼容端点配置（云端大模型）
# ============================================================================
# 支持任何与OpenAI API兼容的服务

# OpenAI官方
OPENAI_ENDPOINT=https://api.openai.com/v1
OPENAI_ENDPOINT_API_KEY=sk-your_openai_api_key_here

# 或者使用其他兼容服务，例如：
# Azure OpenAI
# OPENAI_ENDPOINT=https://your-resource.openai.azure.com/openai/deployments/your-deployment
# OPENAI_ENDPOINT_API_KEY=your_azure_key

# Anthropic Claude (通过兼容层)
# OPENAI_ENDPOINT=https://api.anthropic.com/v1
# OPENAI_ENDPOINT_API_KEY=your_anthropic_key

# 其他云服务商
# OPENAI_ENDPOINT=https://api.your-provider.com/v1
# OPENAI_ENDPOINT_API_KEY=your_provider_key

# ============================================================================
# vLLM本地服务配置
# ============================================================================
# 用于运行本地大语言模型的高性能推理服务

# vLLM服务端点
VLLM_ENDPOINT=http://localhost:8000

# 主要模型名称（用于通用任务）
VLLM_MODEL_NAME=llama-2-7b-chat-hf

# 代码专用模型（可选，用于代码生成任务）
VLLM_CODE_MODEL_NAME=codellama-7b-instruct-hf

# vLLM启动示例命令：
# python -m vllm.entrypoints.openai.api_server \
#   --model meta-llama/Llama-2-7b-chat-hf \
#   --port 8000 \
#   --host 0.0.0.0

# ============================================================================
# LlamaCpp配置（GGUF模型格式）
# ============================================================================
# 用于运行量化的GGUF格式模型

# LlamaCpp服务端点
LLAMACPP_ENDPOINT=http://localhost:8080

# 模型文件路径
LLAMACPP_MODEL_PATH=/path/to/your/model.gguf

# CPU线程数
LLAMACPP_THREADS=4

# GPU层数（如果使用GPU加速）
LLAMACPP_GPU_LAYERS=0

# 是否支持嵌入功能
LLAMACPP_EMBEDDING_MODEL=true

# LlamaCpp启动示例命令：
# ./llama-cpp-python-server \
#   --model /path/to/model.gguf \
#   --port 8080 \
#   --host 0.0.0.0 \
#   --n_threads 4

# ============================================================================
# 传统OpenAI配置（备用）
# ============================================================================
# 如果没有配置上述端点，将使用传统的OpenAI配置

OPENAI_API_KEY=sk-your_traditional_openai_key_here

# ============================================================================
# 模型选择策略配置
# ============================================================================

# 任务类型偏好设置
# PREFER_LOCAL_FOR_WRITING=true
# PREFER_CLOUD_FOR_CODING=true
# PREFER_MULTIMODAL_FOR_BROWSING=true

# 性能偏好
# DEFAULT_PREFER_SPEED=false
# DEFAULT_PREFER_QUALITY=true
# DEFAULT_PREFER_COST=false

# ============================================================================
# 高级配置
# ============================================================================

# Python路径
PYTHONPATH=.

# 日志级别
LOG_LEVEL=INFO

# 模型配置文件路径
MODEL_CONFIG_FILE=config/model_config.json

# 性能监控
ENABLE_PERFORMANCE_MONITORING=true

# 缓存设置
ENABLE_MODEL_RESPONSE_CACHE=true
MODEL_CACHE_TTL_SECONDS=300

# ============================================================================
# 使用说明
# ============================================================================

# 1. 云端大模型配置：
#    - 设置 OPENAI_ENDPOINT 和 OPENAI_ENDPOINT_API_KEY
#    - 用于高质量任务：代码生成、复杂推理、多模态任务

# 2. 本地vLLM配置：
#    - 启动vLLM服务
#    - 设置 VLLM_ENDPOINT 和 VLLM_MODEL_NAME
#    - 用于快速任务：写作、对话、简单分析

# 3. 本地LlamaCpp配置：
#    - 启动LlamaCpp服务
#    - 设置 LLAMACPP_ENDPOINT 和 LLAMACPP_MODEL_PATH
#    - 用于资源受限环境和嵌入任务

# 4. 混合使用：
#    - 可以同时配置多种提供商
#    - 系统会根据任务类型自动选择最适合的模型
#    - 浏览器任务 → 云端多模态模型
#    - 写作任务 → 本地快速模型
#    - 代码任务 → 云端大模型

# 5. 初始化：
#    运行 python initialize_enhanced_models.py 来初始化和测试配置
