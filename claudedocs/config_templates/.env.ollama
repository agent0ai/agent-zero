# Agent Zero Configuration - Pure Ollama (Local, Free)
# Copy this to .env to use local Ollama models exclusively

# ============================================
# Agent Zero Runtime ID (keep existing)
# ============================================
A0_PERSISTENT_RUNTIME_ID=2b028188e762c1f90dc8addb6acbf493

# ============================================
# Primary: Ollama (Local LLM)
# ============================================
# No API key needed! Ollama runs locally
# Make sure Ollama is installed and running: ollama serve
# Pull models: ollama pull qwen2.5-coder:7b && ollama pull gemma2:2b

# Ollama endpoint from Docker container
OLLAMA_API_BASE=http://host.docker.internal:11434

# ============================================
# Backup API Keys (Optional - for fallback)
# ============================================
# Keep these for testing or fallback to cloud models
OPENROUTER_API_KEY=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
GEMINI_API_KEY=AIzaSyBHOLoN8pCBhKXqHqaKmQXqPCYPgPLjxl8

# Add if you have Anthropic API access (separate from Claude subscription!)
# ANTHROPIC_API_KEY=sk-ant-api03-xxxxxxxxxxxxxxxxxxxxx

# ============================================
# Google Cloud (if using Vertex AI)
# ============================================
VERTEX_PROJECT=andre-467020
VERTEX_LOCATION=us-central1

# ============================================
# System Configuration
# ============================================
DEFAULT_USER_UTC_OFFSET_MINUTES=-180
DEFAULT_USER_TIMEZONE=America/Sao_Paulo

# Memory Configuration
MEMORY_SUBDIR=mlcreator

# ============================================
# Model Selection (via Docker Environment)
# ============================================
# Set these in docker-compose-fresh.yml:
# CHAT_MODEL_PROVIDER: "ollama"
# CHAT_MODEL_NAME: "qwen2.5-coder:7b"
# UTIL_MODEL_PROVIDER: "ollama"
# UTIL_MODEL_NAME: "gemma2:2b"
